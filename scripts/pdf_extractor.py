"""
PDF êµì¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° êµ¬ì¡°í™”
Nuevo EspaÃ±ol en Marcha êµì¬ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
"""

import PyPDF2
import sys
import json
import re
from pathlib import Path
from typing import Dict, List

class TextbookExtractor:
    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path
        self.extracted_data = {
            "units": {},
            "vocabulary": {},
            "dialogues": [],
            "exercises": []
        }
    
    def extract_text(self) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text = ""
        with open(self.pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                text += page.extract_text() + "\n"
        
        return text
    
    def parse_unit_structure(self, text: str) -> Dict:
        """êµì¬ ë‹¨ì› êµ¬ì¡° íŒŒì‹±"""
        units = {}
        
        # ë‹¨ì› íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: "Unidad 1", "UNIDAD 1", etc.)
        unit_pattern = r"(?:Unidad|UNIDAD)\s+(\d+)"
        unit_matches = re.finditer(unit_pattern, text, re.IGNORECASE)
        
        for match in unit_matches:
            unit_number = match.group(1)
            unit_start = match.start()
            
            # ë‹¤ìŒ ë‹¨ì› ì‹œì‘ì  ì°¾ê¸°
            next_unit = re.search(unit_pattern, text[unit_start + 10:])
            unit_end = next_unit.start() + unit_start + 10 if next_unit else len(text)
            
            unit_content = text[unit_start:unit_end]
            
            units[f"unidad_{unit_number}"] = {
                "number": unit_number,
                "content": unit_content,
                "sections": self._parse_sections(unit_content)
            }
        
        return units
    
    def _parse_sections(self, unit_text: str) -> Dict:
        """ë‹¨ì› ë‚´ ì„¹ì…˜ íŒŒì‹±"""
        sections = {
            "vocabulary": [],
            "grammar": [],
            "dialogues": [],
            "exercises": []
        }
        
        # ì–´íœ˜ ì„¹ì…˜ ì°¾ê¸°
        vocab_pattern = r"(?:Vocabulario|VOCABULARIO|LÃ©xico)(.*?)(?=\n[A-Z])"
        vocab_matches = re.findall(vocab_pattern, unit_text, re.DOTALL | re.IGNORECASE)
        
        for vocab in vocab_matches:
            words = self._extract_vocabulary(vocab)
            sections["vocabulary"].extend(words)
        
        # ë¬¸ë²• ì„¹ì…˜ ì°¾ê¸°
        grammar_pattern = r"(?:GramÃ¡tica|GRAMÃTICA)(.*?)(?=\n[A-Z])"
        grammar_matches = re.findall(grammar_pattern, unit_text, re.DOTALL | re.IGNORECASE)
        
        for grammar in grammar_matches:
            sections["grammar"].append(self._clean_text(grammar))
        
        return sections
    
    def _extract_vocabulary(self, vocab_text: str) -> List[Dict]:
        """ì–´íœ˜ ì¶”ì¶œ ë° êµ¬ì¡°í™”"""
        vocabulary = []
        
        # ìŠ¤í˜ì¸ì–´-í•œêµ­ì–´ íŒ¨í„´ (ì˜ˆ: "casa - ì§‘")
        word_pattern = r"([a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+)\s*[-â€“]\s*([ê°€-í£]+)"
        matches = re.findall(word_pattern, vocab_text, re.IGNORECASE)
        
        for spanish, korean in matches:
            vocabulary.append({
                "spanish": spanish.strip(),
                "korean": korean.strip(),
                "type": self._guess_word_type(spanish)
            })
        
        return vocabulary
    
    def _guess_word_type(self, word: str) -> str:
        """ë‹¨ì–´ ìœ í˜• ì¶”ì¸¡ (ëª…ì‚¬, ë™ì‚¬ ë“±)"""
        if word.endswith(('ar', 'er', 'ir')):
            return "verb"
        elif word.endswith(('ciÃ³n', 'dad', 'Ã­a')):
            return "noun_feminine"
        elif word.endswith(('o', 'e')):
            return "noun"
        else:
            return "other"
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # í˜ì´ì§€ ë²ˆí˜¸ ì œê±°
        text = re.sub(r'\d+\s*$', '', text)
        return text.strip()
    
    def save_to_json(self, output_path: str):
        """ì¶”ì¶œëœ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.extracted_data, f, ensure_ascii=False, indent=2)
    
    def extract_sample_dialogues(self, text: str) -> List[Dict]:
        """ëŒ€í™”ë¬¸ ì¶”ì¶œ"""
        dialogues = []
        
        # ëŒ€í™” íŒ¨í„´ ì°¾ê¸°
        dialogue_pattern = r"(?:DiÃ¡logo|DIÃLOGO|ConversaciÃ³n)\s*\d*(.*?)(?=\n\n)"
        dialogue_matches = re.findall(dialogue_pattern, text, re.DOTALL | re.IGNORECASE)
        
        for dialogue in dialogue_matches:
            lines = dialogue.strip().split('\n')
            parsed_dialogue = []
            
            for line in lines:
                # í™”ìì™€ ëŒ€ì‚¬ ë¶„ë¦¬ (ì˜ˆ: "MarÃ­a: Hola, Â¿cÃ³mo estÃ¡s?")
                speaker_pattern = r"^([A-Za-zÃ¡Ã©Ã­Ã³ÃºÃ±ÃÃ‰ÃÃ“ÃšÃ‘]+):\s*(.+)"
                match = re.match(speaker_pattern, line.strip())
                
                if match:
                    parsed_dialogue.append({
                        "speaker": match.group(1),
                        "text": match.group(2),
                        "translation": ""  # ë‚˜ì¤‘ì— ë²ˆì—­ ì¶”ê°€
                    })
            
            if parsed_dialogue:
                dialogues.append({
                    "dialogue": parsed_dialogue,
                    "context": "",  # ìƒí™© ì„¤ëª… ì¶”ê°€ ê°€ëŠ¥
                    "unit": ""  # ë‹¨ì› ì •ë³´
                })
        
        return dialogues
    
    def run_extraction(self):
        """ì „ì²´ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸ“š PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        raw_text = self.extract_text()
        
        print("ğŸ” ë‹¨ì› êµ¬ì¡° íŒŒì‹± ì¤‘...")
        self.extracted_data["units"] = self.parse_unit_structure(raw_text)
        
        print("ğŸ’¬ ëŒ€í™”ë¬¸ ì¶”ì¶œ ì¤‘...")
        self.extracted_data["dialogues"] = self.extract_sample_dialogues(raw_text)
        
        print("âœ… ì¶”ì¶œ ì™„ë£Œ!")
        
        # ìš”ì•½ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“Š ì¶”ì¶œ ê²°ê³¼:")
        print(f"- ë‹¨ì› ìˆ˜: {len(self.extracted_data['units'])}")
        print(f"- ëŒ€í™”ë¬¸ ìˆ˜: {len(self.extracted_data['dialogues'])}")
        
        total_vocab = sum(len(unit['sections']['vocabulary']) 
                         for unit in self.extracted_data['units'].values())
        print(f"- ì´ ì–´íœ˜ ìˆ˜: {total_vocab}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê¸°ë³¸ ê²½ë¡œ (ì¸ìë¥¼ í†µí•´ ë®ì–´ì“¸ ìˆ˜ ìˆìŒ)
    pdf_path = "../data/textbook/nuevo_espanol_1.pdf"
    output_path = "../data/textbook/extracted_content.json"

    # ì‚¬ìš©ë²•: python pdf_extractor.py [PDF_PATH] [OUTPUT_JSON_PATH]
    if len(sys.argv) >= 2 and sys.argv[1].strip():
        pdf_path = sys.argv[1]
    if len(sys.argv) >= 3 and sys.argv[2].strip():
        output_path = sys.argv[2]

    # ì¶”ì¶œê¸° ì‹¤í–‰
    extractor = TextbookExtractor(pdf_path)
    extractor.run_extraction()

    # JSONìœ¼ë¡œ ì €ì¥
    extractor.save_to_json(output_path)

    print(f"\nğŸ’¾ ì¶”ì¶œëœ ë°ì´í„°ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
